{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import external libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import internal libraries\n",
    "from melanoma_classification.model import get_dermmel_classifier_v1\n",
    "from utils.dermmel import DermMel\n",
    "from melanoma_classification.utils import (\n",
    "    production_transform, \n",
    "    get_device,\n",
    "    visualize_single_attention,\n",
    "    visualize_multihead_as_single_attention,\n",
    "    visualize_multihead_attention,\n",
    ")\n",
    "from evaluation.evaluator import (\n",
    "    visualize_loss,\n",
    "    visualize_f1_precision_recall,\n",
    "    visualize_accuracy,\n",
    "    create_evaluation_report,\n",
    "    visualize_confusion_matrix,\n",
    "    visualize_model_confidence,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init device\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths\n",
    "figure_path = Path(\"evaluation\") / \"images\"\n",
    "checkpoint_base_path = Path(\"checkpoints\") / \"dermmel_orig_image_test\"\n",
    "training_metrics_filename = \"metrics.csv\"\n",
    "final_model_path = Path(\"..\") / \"src\" / \"melanoma_classification\" / \"weights\"\n",
    "final_model_path.mkdir(exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create & read in model from checkpoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = checkpoint_base_path / \"checkpoint_epoch_20.pth\"\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit = get_dermmel_classifier_v1()\n",
    "vit.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "# Load the model\n",
    "summary(vit, input_size=(1, 3, 224, 224), device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create test dataset & dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = DermMel(\n",
    "    \"../data\", split=\"test\", transform=production_transform()\n",
    ")\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=1, shuffle=False, num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read-in training metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_metrics = pd.read_csv(checkpoint_base_path / training_metrics_filename)\n",
    "training_metrics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_loss(training_metrics, figure_path / \"loss.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_f1_precision_recall(\n",
    "    training_metrics, \n",
    "    figure_path / \"f1_precision_recall.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_accuracy(training_metrics, figure_path / \"accuracy.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_report = create_evaluation_report(\n",
    "    vit, test_dataloader, test_dataset.classes, device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_confusion_matrix(\n",
    "    evaluation_report,\n",
    "    test_dataset.classes,\n",
    "    figure_path / \"confusion_matrix.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_model_confidence(evaluation_report, figure_path / \"model_confidences.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save only the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to production folder\n",
    "torch.save(vit.state_dict(), final_model_path / \"vit.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infer unseen image & visualize attention maps\n",
    "\n",
    "\n",
    "Image source: [Wikipedia](https://en.wikipedia.org/wiki/Melanoma) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and process image\n",
    "img_path = figure_path / \"Melanoma.jpg\"\n",
    "raw_image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "image = (\n",
    "    production_transform()(image=np.array(raw_image))[\"image\"]\n",
    "    .to(device)\n",
    "    .unsqueeze(0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify the image\n",
    "vit.eval()\n",
    "with torch.no_grad():\n",
    "    model_outputs = vit(image)\n",
    "    logits = model_outputs[\"outputs\"]\n",
    "    attention = model_outputs[\"attentions\"]\n",
    "    logits = torch.nn.functional.softmax(logits, 1)\n",
    "    confidence, prediction = torch.max(logits, dim=1)\n",
    "    confidence, prediction = confidence.item(), prediction.item()\n",
    "\n",
    "detected = vit.class_map[prediction]\n",
    "print(f\"Found a {detected} sample with confidence {confidence*100:.2f}%.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the attention maps\n",
    "\n",
    "# Choose layer to visualize\n",
    "layer = -1\n",
    "\n",
    "# Visualize single attention map averaged over all heads and layers\n",
    "visualize_single_attention(\n",
    "    raw_image,\n",
    "    attention,\n",
    "    img_path=img_path / \"single_attention.png\"\n",
    ")\n",
    "\n",
    "# Visualize multihead attention maps as a single attention map for a specific\n",
    "# layer\n",
    "visualize_multihead_as_single_attention(\n",
    "    raw_image,\n",
    "    attention,\n",
    "    layer=layer,\n",
    "    img_path=img_path / f\"multihead_as_single_attention{layer}.png\"\n",
    ")\n",
    "\n",
    "# Visualize multihead attention maps for a specific layer separately for each\n",
    "# head\n",
    "visualize_multihead_attention(\n",
    "    raw_image, \n",
    "    attention, \n",
    "    img_path=img_path / f\"multihead_attention{layer}.png\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "melanoma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
